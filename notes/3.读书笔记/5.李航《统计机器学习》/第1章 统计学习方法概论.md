## 第1章 统计学习方法概论
本章简要叙述统计学习方法的一些基本概念.这是对全书内容的概括,也是全书内容的基础.首先叙述统计学习的定义,研究对象与方法;然后叙述统计学习,这是本书的主要内容;接着提出统计学习方法的三要素:模型,策略,算法;介绍模型选择,包括正则化,交叉验证与学习的泛化能力;介绍生成模型和判别模型;最后介绍监督学习方法的应用:分类问题,标注问题和回归问题.

### 1.1 统计学习

##### 1. 统计学习的特点
1. 统计学习以计算机及网络为平台,建立在计算机及网络上的;
2. 统计学习以数据为研究对象,是数据驱动的学科;
3. 统计学习的目的是对数据进行预测与分析;
4. 统计学习以方法为中心,统计学习方法构建模型并应用模型进行预测与分析;
5. 统计学习是概率论,统计学,信息论,计算理论,最优化理论及计算机科学等多个领域的交叉学科,并且在发展中逐步形成独立的理论体系与方法论.

赫尔伯特·西蒙(Herbert A. Simon)曾对"学习”给出以下定义:"如果一个系统能够通过执行某个过程改进它的性能,这就是学习.”统计学习就是计算机系统通过运用数据以及统计方法提高系统性能的机器学习.现在,机器学习一般就是指统计机器学习.

##### 2. 统计学习的对象
对象是数据(data).从数据出发,提取数据的特征,抽象出数据的模型,发现数据中的知识,又回到对数据的分析与预测中去.数据是多样的,各种数字,文字,图像,视频,音频以及它们的组合.
统计学习关于数据的基本假设是同类数据具有一定的统计规律性,可以用概率统计的方法来加以处理.例如:可以用随机变量描述数据中的特征,用概率分布描述数据的统计规律.

##### 3. 统计学习的目的
统计学习用于对数据进行预测与分析,特别是对位置新数据进行预测与分析.对数据的预测分析是通过构建概率统计模型实现的.所以,统计学习总的目标就是考虑学习什么样的模型和如何学习模型,以使模型能对数据进行准确的预测分析,同事也要考虑尽可能地提高学习效率.

##### 4. 统计学习的方法
统计学习的方法是基于数据构建统计模型从而对数据进行预测分析.统计学习有监督学习(supervised learning),费监督学习(unsupervised learning),半监督学习(semi-supervised learning)和强化学习(reinforcement learning)等组成.
这本书主要讨论监督学习,这种情况下统计学习的方法可以概况为:从给定的,有限的,用于学习的训练数据(training data)集合出发,假设数据是独立同分布产生的;并且假设要学习的模型属于某个函数的集合,称为假设空间(hypothesis space);应用某个评价准则(evaluation criterion),从假设空间中选取一个最优的模型,使它对已知训练数据以及未知测试数据(test data)在给定的评价准则下有最优的预测;最优模型的选取由算法实现.这样,统计学习方法包括模型的假设空间,模型选择的准则以及模型学习的算法,称其为统计学习方法的三要素:模型(model),策略(strategy)和算法(algorithm).

实现统计学习方法的步骤如下:
1. 得到一个有限的训练数据结婚;
2. 确定包含所有可能的模型的假设空间,即学习模型的结婚;
3. 确定模型选择的准则,即学习的策略;
4. 实现求解最优模型的算法,即学习的算法;
5. 通过学习方法选择最优模型;
6. 利用学习的最优模型对新数据进行预测分析.

##### 5. 统计学习的研究
统计学习研究一般包括统计学习方法(statistical learning method),统计学习理论(statistical learning theory)及统计学习应用(application of statistical learning)三个方面.

##### 6. 统计学习的重要性
近20年来,统计学习无论是在理论还是在应用上都取得了巨大的发展,有许多重大突破,已成功应用到人工智能,模式识别,数据挖掘,自然语言处理,语音识别,图像识别,信息检索和生物信息等许多计算机应用领域中,并且成为核心技术.
统计学习学科在科学技术中的重要性主要体现在以下几个方面:
1. 统计学习是处理海量数据的有效方法.
2. 统计学习是计算机智能化的有效手段.
3. 统计学习是计算机科学发展的一个重要组成部分.

### 1.2 监督学习
统计学习包括监督学习,费监督学习,半监督学习及强化学习.本书朱啊哟讨论监督学习.
监督学习(supervised learning)的任务是学习一个模型,使模型能够对任意给定的输入,对其相应的输出做出一个好的预测.

#### 1.2.1 基本概念
##### 1. 输入空间,特征空间与输出空间
在监督学习中,将输入与输出所有可能取值的集合分别成为输入空间(input space)和输出空间(output space).输入与输出空间可以使有限元数的集合,也可以是整个欧氏空间.可以使同一空间,也可以是不同空间;但通常输出空间远远小于输入空间.  
每个具体的输入是一个实例(instance),通常由特征向量(feature vector)表示.这是,所有特征向量存在的空间成为特征空间(feature space).特征空间的每一维对应一个特征.又是假设输入空间与特征空间是相同的空间,对他们不加区分;有时假设输入空间与特征空间为不同的空间,将实例从输入空间映射到特征空间(SVM).模型实际上都是定义在特征空间上的.  
在监督学习过程中,将输入与输出看作是定义在输入(特征)空间与输出空间上的随机变量的曲子.输入,输出变量用大写字母表示,习惯上输入变量写作X,输出变量写作Y.输入,输出变量索取的值用小写字母表示,输入x,输出y.变量可以使标量或向量.除特别声明外,书中向量皆为列向量,输入实例x的特征向量记为:

$$
    \boldsymbol{x} = (x^{(1)},x^{(2)},...,x^{(i)},...,x^{(n)})^T
$$

$x^{(i)}$表示x的第i个德政.注意,$x^{(i)}$与$x_i$不同,本书通常用$x_i$表示多个输入变量中的第i个,即:

$$
    \boldsymbol{x_i} = (x_i^{(1)},x_i^{(2)},...,x_i^{(i)},...,x_i^{(n)})^T
$$

监督学习从训练数据(training data)集合中学习模型,对测试数据(test data)进行预测.训练数据由输入(或特征向量)与输出对组成,训练集通常表示为:

$$
    T = {(\boldsymbol{x_1},y_1),(\boldsymbol{x_2},y_2),...,(\boldsymbol{x_N},y_N)}
$$

测试数据也由输入输出对组成.输入输出对又称为样本(sample)或样本点.  
输入变量X和输出变量Y有不同的类型,可以是连续的,也可以是离散的.人们根据输入,输出变量的不同类型,对预测任务与给予不同的名称:输入输出变量均为连续的预测问题成为<strong>回归问题</strong>;输出变量为有限个离散变量成为<strong>分类问题</strong>;输入输出变量均为变量序列成为<strong>标注问题</strong>.

##### 2. 联合概率分布
监督学习假设输入输出的随机变量X和Y遵循联合概率分布P(X,Y).P(X,Y)表示分布函数,或分布密度函数.注意,是假设联合概率分布存在,但具体定义是未知的.训练数据与测试数据被看做是依联合概率分布P(X,Y)独立同分布产生的.统计学习假设数据存在一定的统计规律,<strong>X和Y具有联合概率分布的假设就是监督学习关于数据的基本假设</strong>.

##### 3. 假设空间
监督学习的目的是学习一个由输入到输出的映射,这一映射由模型来表示.模型属于由输入空间到输出空间的映射的集合.这个集合即假设空间(hypothesis space).  
监督学习的模型可以是概率模型或非概率模型,由条件概率P(Y|X)或决策函数(decision function)Y=f(X)表示,随具体学习方法而定.对具体的输入进行相应的输出预测时,写作P(y|x)或y=f(x).

#### 1.2.2 问题的形式化
```mermaid
graph LR
    开始-->学习系统-->模型<-->预测系统
```
首先给出一个训练数据集

$$
    T = {(\boldsymbol{x_1},y_1),(\boldsymbol{x_2},y_2),...,(\boldsymbol{x_N},y_N)}
$$
其中$(\boldsymbol{x_i},y_i),i=1,2,...,N$称为样本或样本点.$\boldsymbol{x_i}\in \mathcal{X}\subseteq\mathbb{R}^n$是输入,$y_i\in\mathcal{Y}$是输出.  
监督学习中,假设训练数据与测试数据是依联合概率分布$P(X,Y)$独立同分布产生的.  
在学习过程中,学习系统利用给定的训练数据集,学习得到的模型,表示为条件概率分布$\hat{P}(Y|X)$或决策函数$Y=\hat{f}(X)$.  
在预测过程中,预测系统对于给定的测试样本集中的输入$x_{N+1}$,由模型$y_{N+1}=\argmax_{y_{N+1}}\hat{P}(y_{N+1}|x_{N+1})或y_{N+1}=\hat{f}(x_{N+1})$给出相应的输出$y_{N+1}$.
### 1.3 统计学习三要素
统计学习方法都是有模型,决策和算法构成的,即统计学习方法由三要素构成,简单表示为:

$$
    方法=模型+策略+算法
$$

#### 1.3.1 模型
在监督学习过程中,模型就是所要学习的条件概率分布或决策函数.模型的假设空间(hypothesis space)包含所有可能的条件概率分布或决策函数.例如,假设决策函数是输入变量的线性函数,那么模型的假设空间就是所有这些线性函数构成的函数集合.假设空间中的模型一般有无穷个.  
假设空间用$\mathcal{F}$表示.假设空间可以定义为决策函数的集合

$$
    \mathcal{F}=\{f|Y=f(X)\}
$$

其中,X和Y是定义在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$上的变量.这时$\mathcal{F}$通常是由一个参数向量决定的函数族:

$$
    \mathcal{F}=\{f|Y=f_{\boldsymbol{\theta}}(X),\boldsymbol{\theta}\in \mathbb{R}^n\}
$$

参数向量$\boldsymbol{\theta}$取值于n维欧氏空间$\mathbb{R}^n$,称为参数空间(parameter space).  
假设空间也可以定义为条件概率的集合

$$
    \mathcal{F}=\{P|P(Y|X)\}
$$

其中,X和Y是决定在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$上的随机变量.这时$\mathcal{F}$通常是由一个参数向量决定的条件概率分布族:

$$
    \mathcal{F}=\{P|P_{\boldsymbol{\theta}}(Y|X),\boldsymbol{\theta}\in \mathbb{R}^n\}
$$

参数向量$\boldsymbol{\theta}$取值于n维欧氏空间$\mathbb{R}^n$,称为参数空间(parameter space).  
书中把决策函数表示的模型称为非概率模型,把条件概率表示的模型称为概率模型.
#### 1.3.2 策略
统计学习的目标:从假设空间中选取最优模型.  
首先引入损失函数与风险函数的概念.损失函数度量模型一次预测的好坏,风险函数度量平均意义下模型预测的好坏.  
##### 1. 损失函数和风险函数
考虑决策函数$f$,对于给定的输入$X$,给出的输出$f(X)$与真实值$Y$可能一致也可能不一致,用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度.损失函数是$f(X)$和$Y$的非负实值函数,记为$L(Y,f(X))$.  
统计学习常用的损失函数:  
1. 0-1损失函数(0-1 loss function) 
   
$$
    L(Y,f(X))=\begin{cases}1,\quad Y\neq f(X)\\0,\quad Y=f(X)\end{cases} 
$$

2. 平方损失函数(quadratic loss function)
      
$$
    L(Y,f(X))=(Y-f(X))^2
$$

3. 绝对损失函数(absolute loss function)

$$
    L(Y,f(X))=|Y-f(X)|
$$

4. 对数损失函数(logarithmic loss function)或对数似然损失函数(loglikelihood loss function)
   
$$
    \begin{aligned}
         L(Y,P(Y|X))&=-\log{P(Y|X)}
         \\&=-\log{\prod_{i=1}^N\prod_{j=1}^Mp_{ij}^{y_{ij}}}
         \\&=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log{p_{ij}}
    \end{aligned}
$$

其中,N为输入样本量,M为可能的类别数,$y_{ij}$是一个二值指标,表示类别j是否是输入实例$x_i$的真实类别.$p_{ij}$为模型预测输入实例$x_i$属于类别$y_j$的概率.  

损失函数值越小越好,由于模型的输入输出对$(X,Y)$是随机变量,遵循联合分布$P(X,Y)$,所以损失函数的期望是

$$
    R_{exp}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X}\times\mathcal{Y}}L(y,f(x))P(x,y)dxdy
$$

这是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失,简称为风险函数(risk function)或期望损失(expected loss).  
学习的目的就是选择期望风险最小的模型.由于联合分布$P(X,Y)$是未知的,$R_{exp}(f)$不能直接计算.实际上,如果知道联合分布$P(X,Y)$,可以从联合分布直接求出条件概率分布$P(Y|X)$,也就不需要学习了.正因为不知道联合概率分布,所以才需要进行学习.这样一来,一方面根据期望风险最小学习模型要用到联合分布,另一方面联合分布又是未知的,所以监督学习就成为一个病态问题(ill-formed problem).  
给定一个训练数据集

$$
    T = {(\boldsymbol{x_1},y_1),(\boldsymbol{x_2},y_2),...,(\boldsymbol{x_N},y_N)}
$$

模型$f(X)$关于训练数据集的平均损失成为经验风险(empirical risk)或经验损失(empiricalloss),记为:

$$
    R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
$$

期望风险$R_{exp}(f)$是模型关于联合分布的期望损失,经验风险$R_{emp}(f)$是模型关于训练样本及的平均损失.根据大数定律,当样本容量N趋于无穷时,经验风险R_{emp}(f)$趋于期望风险R_{exp}(f)$.所以可以用经验风险估计期望风险.  
但是,现实中训练样本数目有限,甚至很小,所以用经验风险估计期望风险常常并不理想,要对经验风险进行一定的矫正.这就关系到监督学习的两个基本策略:经验风险最小化和结构风险最小化.
##### 2. 经验风险最小化和结构风险最小化
在假设空间,损失函数以及训练数据集确定的情况下,经验风险函数$R_{exp}$就可以确定.经验风险最小化(empirical risk minimization,ERM)的策略认为:经验风险最小的模型就是最优的模型.按经验风险最小化求最优模型就是求解最优化问题:

$$
    \min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
$$

其中,$\mathcal{F}$是假设空间.  
当样本容量足够大时,经验风险最小化能保证有很好的学习效果,在现实中被广泛采用.比如:极大似然估计(maximum likelihood estimation,MLE)就是经验风险最小化的一个例子.当模型是条件概率分布,损失函数是对数损失函数时,经验风险最小化就是极大似然估计.  
但是,当样本容量很小时,经验风险最小化学习的效果就未必很好,会产生后面将要叙述的"过拟合(over-fitting)"现象.  
结构风险最小化(structural risk minimization,SRM)是为了防止过拟合而提出来的策略.结构风险最小化等价于正则化(regularization).结构风险在经验风险上加了表示模型复杂度的正则化项(regularizer)或罚项(penalty term).在假设空间,损失函数以及训练数据集确定的情况下,结构风险的定义是:

$$
    R_{srm}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
$$

其中,$J(f)$表示模型的复杂度,是定义再假设空间$\mathcal{F}$上的泛函.模型$f$越复杂,复杂度$J(f)$就越大.复杂度表示了复杂模型的惩罚(可以看成是将:"若非必要,勿增实体”这句话的信息使用到风险函数中).$\lambda\geq0$是系数,用来权衡经验风险和模型复杂度.结构风险小需要经验风险与模型复杂度同时小.结构风险小的模型往往对训练数据以及位置的测试数据都有较好的预测. 
比如,贝叶斯估计中的最大后验概率估计(maximum posterior probability estimation, MAP)就是结构风险最小化的一个例子.当模型是条件概率分布,损失函数是对数损失函数,模型复杂度由模型的先验概率表示时,结构风险最小化就是最大后验概率估计.  
结构风险最小化(structural risk minimization,ERM)的策略认为:结构风险最小的模型就是最优的模型.按结构风险最小化求最优模型就是求解最优化问题:

$$
    \min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
$$

其中,$\mathcal{F}$是假设空间.  
综上,监督学习问题转化为经验风险或结构风险函数的最优化问题.这时经验或结构风险函数是最优化的目标函数.
#### 1.3.3 算法
从假设空间选择最优模型,需要考虑用什么计算方法来求解最优模型.统计过学习问题轨迹为最优化问题,统计学习的算法成为求解最优化问题的算法.如果最优化问题有显式的解析解,这个最优化问题就比较简单.但通常解析解不存在,需要数值计算的方法求解.如何保证找到全局最优解,并使求解的过程非常高效,是重要的.统计学习一般使用已有的最优化算法,有时也需要开发独自的算法.
### 1.4 模型评估与模型选择
#### 1.4.1 训练误差与测试误差
统计学习方法的目的是使学到的模型不仅对已知数据,而且对位置数据都能有很好的预测能力.不同的学习方法会给出不同的模型.当损失函数给定时,基于损失函数的模型的训练误差(training error)和模型的测试误差(test error)就自然成为学习方法评估的标准.统计学习方法具体采用的损失函数未必是评估时采用的损失函数.  
假设学习到的模型是$Y=\hat{f}(X)$,训练误差是模型$Y=\hat{f}(X)$关于训练数据集的平均损失:

$$
    R_{emp}(\hat{f})=\frac{1}{N}\sum_{i=1}^NL(y_i,\hat{f}(x_i))
$$

其中N是训练样本容量.  
测试误差是模型$Y=\hat{f}(X)$关于测试数据集的平均损失:

$$
    e_{test}(\hat{f})=\frac{1}{N'}\sum_{i=1}^{N'}L(y_i,\hat{f}(x_i))
$$

其中N'是测试样本容量.  
例如,当损失函数是0-1损失时,测试误差就变成了常见的测试数据及上的误差率(error rate)

$$
    e_{test}(\hat{f})=\frac{1}{N'}\sum_{i=1}^{N'}I(y_i\neq\hat{f}(x_i))
$$

这里I是指示函数(indicator function),即$y_i\neq\hat{f}(x_i)$时为1,否则为0.  
相应的,常见的测试数据集上的准确率(accuracy)为

$$
    r_{test}(\hat{f})=\frac{1}{N'}\sum_{i=1}^{N'}I(y_i=\hat{f}(x_i))
$$

显然

$$
    e_{test}+r_{test}=1
$$

训练误差的大小,对判断问题是不是一个容易学习的问题是有意义的,但本质不重要.测试误差反应了学习方法对未知测试数据集的预测能力,是学习中的重要概念.显然,给定两种学习方法,测试误差小的方法具有更好的预测能力,是更有有效的方法.通常将学习方法对位置数据的预测能力成为泛化能力(generalization ability).
#### 1.4.2 过拟合与模型选择

### 1.5 正则化与交叉验证

#### 1.5.1 正则化

#### 1.5.2 交叉验证

### 1.6 泛化能力

#### 1.6.1 泛化误差

#### 1.6.2 泛化误差上界

### 1.7 生成模型与判别模型

### 1.8 分类问题

### 1.9 标注问题
标注(tagging)也是一个监督学习问题.可以将标注问题看成是分类问题的一个推广.

标注问题又是更加复杂的**结构预测(structure prediction)**问题的简单形式.

结构预测又是啥?更加复杂?

分类→标注→结构预测



### 1.10 回归问题


