
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
<!-- 此处省略掉markdown的css样式，因为太长了 -->
</style>
<h2>统计中的各种"差"</h2>
<h3>一 数理统计中的各种"差"</h3>
<p>统计学中有许多"差",包括:误差,偏差,离差,方差,标准差,残差.
偏差和离差是同一概念,英文是deviation.标准差,均方差和标准偏差是同一个概念,英文是standard deviation,简称std dev.</p>
<h4>1.1 误差</h4>
<h4>1.2 偏差</h4>
<h5><a href="https://wiki.mbalib.com/wiki/偏差">偏差</a></h5>
<h6>什么是偏差</h6>
<p>偏差(Deviation)偏差又称为表观误差,是指个别测定值与测定的平均值之差,它可以用来衡量测定结果的精密度高低.在统计学中,偏差可以用于两个不同的概念,即有偏采样与有偏估计.一个有偏采样是对总样本集非平等采样,而一个有偏估计则是指高估或低估要估计的量.</p>
<p>偏差不一定有害.尽管一个有偏采样会难以分析或引起不准确甚至错误的推断,但是有偏估计在某些情况下也有一些好的特性,例如较小的方差.</p>
<p>每个样本都有一个偏差,有多少样本就有多少偏差.</p>
<h6>偏差的分类</h6>
<p>偏差分为绝对偏差和相对偏差,标准偏差和相对标准偏差来表示.
1. <a href="https://wiki.mbalib.com/wiki/绝对偏差">绝对偏差</a>:是指某一次测量值与平均值的差异.
2. <a href="https://wiki.mbalib.com/wiki/相对偏差">相对偏差</a>:是指某一次测量的绝对偏差占平均值的百分比.
3. <a href="https://wiki.mbalib.com/wiki/平均偏差">平均偏差</a>:是指单项测定值与平均值的偏差(取绝对值)之和,除以测定次数.
4. <a href="https://wiki.mbalib.com/wiki/相对平均偏差">相对平均偏差</a>:是指平均偏差占平均值的百分率.
5. <a href="https://wiki.mbalib.com/wiki/标准偏差">标准偏差</a>:是指统计结果在某一个时段内误差上下波动的幅度.
6. <a href="https://wiki.mbalib.com/wiki/相对标准偏差">相对标准偏差</a>:是指标准偏差占平均值的百分率.</p>
<p>绝对偏差和相对偏差有正有负.
平均偏差,相对平均偏差,标准偏差和相对标准偏差都是正值.</p>
<h6>偏差与精密度</h6>
<p><a href="https://wiki.mbalib.com/wiki/精密度">精密度</a>是指一样品多次平行测定结果之间的符合程度,用偏差表示.偏差越小,说明测定结果精密度越高.</p>
<h6>偏差与误差</h6>
<p><a href="https://wiki.mbalib.com/wiki/误差">误差</a>是测量值与真值之间的差值.用误差衡量测量结果的<a href="https://wiki.mbalib.com/wiki/准确度">准确度</a>,用偏差衡量测量结果的<a href="https://wiki.mbalib.com/wiki/精密度">精密度</a>;误差是以<strong>真实值</strong>为标准,偏差是以多次测量结果的<strong>平均值</strong>为标准.</p>
<p>误差与偏差的含义不同,必须加以区别.但是由于在一般情况下,真实值是不知道的(测量的目的就是为了测得真实值),因此处理实际问题时常常在尽量减小<a href="https://wiki.mbalib.com/wiki/系统误差">系统误差</a>的前提下,把多次平行测量值当作真实值,把偏差当作误差.</p>
<h6>偏差的实例</h6>
<p>例:分析铁矿石中铁的质量分数,得到如下数据:$37.45,37.20,37.50,37.30,37.25(\%)$,计算测结果的平均值,平均偏差,相对平均偏差,标准偏差.
解:
　　<strong>平均值</strong>:$\bar{x}=37.34\%$
　　各次测量的<strong>偏差(即绝对偏差)</strong>$d_i=x_i-\bar{x}$分别是:
　　$0.11,-0.14,0.16,-0.04,-0.09$
　　各次测量的<strong>相对偏差</strong>$d_{ri}=\frac{d_i}{\bar{x}}×100\%=\frac{x_i-\bar{x}}{\bar{x}}×100\%$分别是:
　　$0.29\%,-0.37\%,0.43\%,-0.11\%,-0.24\%$
　　<strong>平均偏差</strong>:$\bar{d}=\frac{\sum^{n}<em>{i=1}|x_i-\bar{x}|}{n}=(0.11+0.14+0.04+0.16+0.09)/5=0.11\%$
　　<strong>相对平均偏差</strong>:$\frac{\bar{d}}{\bar{x}}\times 100\%=0.11/37.34=0.29\%$
　　<strong>标准偏差</strong>:$S=\sqrt{\frac{\sum^{n}</em>{i=1}\left(x_i-\bar{x}\right)^2}{n-1}}$<br />
　　$=\sqrt{\frac{(37.45\%-37.34\%)^2+(37.20\%-37.34\%)^2+(37.50\%-37.34\%)^2+(37.30\%-37.34\%)^2+(37.25\%-37.34\%)^2}{5-1}}$
　　=0.13\%
　　<strong>相对标准偏差:</strong>$RSD=(0.13/37.34)×100\%=0.4\%$</p>
<h5><a href="https://baike.baidu.com/item/标准偏差">标准偏差</a></h5>
<p>标准偏差(Std Dev,Standard Deviation)就是<a href="https://baike.baidu.com/item/标准差">标准差</a>,在使用时注意是总体标准偏差还是样本标准偏差.</p>
<h4>1.3 方差</h4>
<h5><a href="https://baike.baidu.com/item/方差">方差</a></h5>
<p>方差(variance)是在概率论和统计中衡量随机变量或一组数据时离散程度的度量.概率论中方差用来度量随机变量和其数学期望(即均值)之间的偏离程度.统计中的方差(样本方差)是每个样本值与全体样本值的平均数之差的平方值的平均数.在许多实际问题中,研究方差即偏离程度有着重要意义.</p>
<p>方差是衡量源数据和期望值相差的度量值,刻画了随机变量的取值对于其数学期望的离散程度.(标准差,方差越大,离散程度越大)</p>
<p>方差在统计描述和概率分布中各有不同的定义,并有不同的公式.</p>
<p>在统计描述中,方差用来计算每一个变量(观察值)与总体均数之间的差异.为避免出现离均差总和为零,离均差平方和受样本含量的影响,统计学采用平均离均差平方和来描述变量的变异程度.总体方差计算公式:
$$\sigma^2=\frac{1}{N}\sum(X-\mu)^2$$
$\sigma^2$是总体方差,$X$为变量,$\mu$为总体均值,$N$为总体例数.</p>
<p>实际工作中,总体统计量难以得到时,应用样本统计量代替总体参数,经校验后,得到样本方差计算公式:
$$S^2=\frac{1}{n-1}\sum(X-\bar{X})^2$$
$S^2$是样本方差,$X$为变量,$\bar{X}$为样本均值,$n$为样本例数.</p>
<h6>更一般的表达式:</h6>
<p>$$D(X)=E{[X-E(X)]^2}=E(X^2)-[E(X)]^2$$</p>
<h5><a href="https://baike.baidu.com/item/标准差">标准差</a></h5>
<p>标准差(Standard Deviation),中文环境中又常称<strong>均方差</strong>,有时也称<strong>标准偏差</strong>,是离均差平方的算术平均数的平方根,用$\sigma$表示.在概率统计中最常使用作为统计分布程度上的测量.标准差是方差的算术平方根.标准差能反映一个数据集的离散程度.平均数相同的两组数据,标准差未必相同.</p>
<p>所有数减去其平均值的平方和,所得结果除以该组数之个数(或个数减一,即变异数),再把所得值开根号,所得之数就是这组数据的标准差.即$\sigma=\sqrt{D(X)}$</p>
<p>它与$X$有相同的量纲,用来衡量一组数据的离散程度的统计量.</p>
<h6>标准差意义</h6>
<p>由于方差是数据的平方,与检测值本身相差太大,人们难以直观的衡量,所以常用方差开根号换算回来这就是我们要说的标准差.
在统计学中样本的均差多是除以自由度n-1,它是意思是样本能自由选择的程度.当选到只剩一个时,它不可能再有自由了,所以自由度是n-1.</p>
<p>标准差能很客观准确的反映一组数据的离散程度,但是对于不同的项目,或同一项目不同的样本,标准差就缺乏可比性了,因此对于方法学评价来说又引入了变异系数$c_v$.</p>
<h5><a href="https://baike.baidu.com/item/变异系数">变异系数</a></h5>
<p>变异系数(Coefficient of Variation):当需要比较两组数据离散程度大小的时候,如果两组数据的测量尺度相差太大,或者数据量纲的不同,直接使用标准差来进行比较不合适,此时就应当消除测量尺度和量纲的影响,而变异系数可以做到这一点,它是原始数据标准差与原始数据平均数的比.$c_v$没有量纲,这样就可以进行客观比较了.事实上,可以认为变异系数和极差,标准差和方差一样,都是反映数据离散程度的绝对值.其数据大小不仅受变量值离散程度的影响,而且还受变量值平均水平大小的影响.</p>
<p>在概率论和统计学中,变异系数,又称"离散系数"(英文:coefficient of variation),是概率分布离散程度的一个归一化量度,其定义为标准差与平均值之比:
$$c_v=\frac{\sigma}{\mu}$$
变异系数(coefficient of variation)只在平均值不为零时有定义,而且一般适用于平均值大于零的情况.变异系数也被称为标准离差率或单位风险.</p>
<h5><a href="https://baike.baidu.com/item/方差分析">方差分析</a></h5>
<p>方差分析(Analysis of Variance,简称ANOVA),又称"<a href="https://baike.baidu.com/item/变异数分析">变异数分析</a>"或"F检验",是R.A.Fisher发明的,用于两个及两个以上样本均数差别的<a href="https://baike.baidu.com/item/显著性检验">显著性检验</a>. 由于各种因素的影响,研究所得的数据呈现波动状.造成波动的原因可分成两类,一是不可控的随机因素,另一是研究中施加的对结果形成影响的可控因素</p>
<h6>基本原理</h6>
<p>方差分析的基本原理是认为不同处理组的均数间的差别基本来源有两个:
1. 实验条件,即不同的处理造成的差异,称为组间差异.用变量在各组的均值与总均值之偏差平方和的总和表示,记作SSb,组间自由度dfb.
2. 随机误差,如测量误差造成的差异或个体间的差异,称为组内差异,用变量在各组的均值与该组内变量值之偏差平方和的总和表示, 记作SSw,组内自由度dfw.</p>
<p>总偏差平方和 SSt = SSb + SSw.
组内SSw,组间SSb除以各自的自由度(组内dfw =n-m,组间dfb=m-1,其中n为样本总数,m为组数),得到其均方MSw和MSb,一种情况是处理没有作用,即各组样本均来自同一总体,MSb/MSw≈1.另一种情况是处理确实有作用,组间均方是由于误差与不同处理共同导致的结果,即各样本来自不同总体.那么,MSb&gt;&gt;MSw(远远大于).
MSb/MSw比值构成F分布.用F值与其临界值比较,推断各样本是否来自相同的总体</p>
<h4>1.4 残差</h4>
<h5><a href="https://baike.baidu.com/item/残差">残差</a></h5>
<p>残差(residual)在数理统计中是指实验观察值与估计值(拟合值)之间的差.</p>
<p>残差混喊了有关模型基本假设的重要信息.如果回归模型正确的话,我们可以将残差看做误差的观测值.</p>
<p>如果有n个样本就有n个残差,其计算公式为: $y_i-\hat{y_i}$</p>
<p>在集成学习中可以通过基模型拟合残差,使得集成的模型变得更精确;在深度学习中也有人利用layer去拟合残差讲深度神经网络的性能提高变强.在机器学习中的应用可以看<a href="http://www.jianshu.com/p/cd3be2fcc8a3">GBDT和Resnet的原理</a></p>
<h5><a href="https://baike.baidu.com/item/残差平方和">残差平方和</a></h5>
<p>$SSR=\sum_{i=1}^n(y_i-\hat{y_i})^2$
残差平方和是在线性模型中衡量模型拟合程度的一个量,用连续曲线近似地刻画或比拟平面上离散点组,以表示坐标之间函数关系的一种数据处理方法.用解析表达式逼近离散数据的一种方法.</p>
<p>为了明确解释变量和随机误差各产生的效应是多少,统计学上把数据点与它在回归直线上相应位置的差异称为残差,把每个残差平方之后加起来 称为残差平方和,它表示随机误差的效应.一组数据的残差平方和越小,其拟合程度越好.</p>
<h5><a href="https://baike.baidu.com/item/残差分析">残差分析</a></h5>
<p>残差分析(residual analysis): 通过残差所提供的信息,分析出数据的可靠性,周期性或者其他干扰,是一种用于分析模型的假设正确与否的方法.
其一般内容包括: 异常值检验,方差齐性检验,误差的正态性检验,相关性检验以及相伴随的方差稳定化变换,正态变化等修正方法.</p>
<h4>1.4 残差,方差,偏差总结</h4>
<ul>
<li>简单模型:偏差大,方差小(简单模型受样本值的影响小,稳定性高),容易造成欠拟合</li>
<li>复杂模型:偏差小,方差大,容易造成过拟合</li>
<li>判断偏差大还是方差大:</li>
<li>模型上的训练样本的真实值较少,则偏差大(欠拟合)</li>
<li>在训练样本拟合的较好,但在测试样本拟合较差,则方差大(过拟合)</li>
<li>当偏差较大时,表示目标可能未在模型上(即未瞄准靶心),需要冲洗你训练模型(有可能未考虑其他因素
    对样本的影响,或者应让模型更加复杂考虑更高次幂的情况).增加网络层数,增加隐藏层神经元数量,增
    加算法迭代次数,或者用更好的优化算法.</li>
<li>当方差较大时,增加更多的数据或者正则化</li>
</ul>
<h4>1.5 MSE,RMSE,MAE</h4>
<p>MSE(Mean Square Error)均方误差
RMSE(Root Mean Square Error)均方根误差 $RMSE=\sqrt{MSE}$
- MSE
  - 定义:参数估计中均方误差是指参数估计值与参数真值之差平方的期望值,记为MSE
  - 均方误差合成
    - 如果只有一组数据,样本方差就是均方误差
    - 如果N个数据分为r组,且第$i$组的样本方差为$s_i^{2}$ ,则全体的均方差为
    $MSE=\frac{\sum_{i=1}^{r}(n_i-1)s_i^{2}}{N-r}$      <br />
    其中,分子为误差平方和,分母为自由度</p>
<h3>二 统计学习中的三种误差</h3>
<p>统计学习的目的是使学到的模型不仅对已知数据,而且对位置数据都能有很好的预测能力.不同的学习方法会给出不同的模型.当损失函数给定时,基于损失函数的模型的训练误差(training error)和模型的测试误差(test error)就自然成为学习方法评估的标准.注意,统计学习方法具体采用的损失函数未必是评估时使用的损失函数.当然,让两者一直是比较理想的.</p>
<h4>2.1 训练误差</h4>
<p>假设学习到的模型是$Y=\hat{f}(X)$,训练误差是模型$Y=\hat{f}(X)$关于训练数据集的平均损失:
$$R_{emp}(\hat{f})=\frac{1}{N}\sum_{i=1}^NL(y_i,\hat{f}(x_i))$$
其中$N$是训练样本容量.</p>
<h4>2.2 测试误差</h4>
<p>测试误差是模型$Y=\hat{f}(X)$关于测试数据集的平均损失:
$$e_{test}(\hat{f})=\frac{1}{N^\prime}\sum_{i=1}^{N^\prime}L(y_i,\hat{f}(x_i))$$
其中$N^\prime$是训练样本容量.
机器学习模型在训练数据集上表现出的误差叫做训练误差;</p>
<h4>2.3 泛化误差</h4>
<p>在任意一个测试数据样本上表现出的误差的期望值叫做泛化误差
统计学习理论的一个假设是:训练数据集和测试数据集里的每一个数据样本都是从同一个概率分布中相互独立地生成出的(独立同分布假设).
基于以上独立同分布假设,给定任意一个机器学习模型及其参数,它的训练误差的期望值和泛化误差都是一样的.然而从之前的章节中我们了解到,在机器学习的过程中,模型的参数并不是事先给定的,而是通过训练数据学习得出的:模型的参数在训练中使训练误差不断降低.
所以,如果模型参数是通过训练数据学习得出的,那么训练误差的期望值无法高于泛化误差.换句话说,通常情况下,由训练数据学到的模型参数会使模型在训练数据上的表现不差于在测试数据上的表现.
结论:训练误差的降低不一定意味着泛化误差的降低.机器学习既需要降低训练误差,又需要降低泛化误差.
- 1
- 2
- 泛化误差的偏差方差分解</p>